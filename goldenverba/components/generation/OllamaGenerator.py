import os

import json
import asyncio
import aiohttp
import runpod
import logging

from goldenverba.components.interfaces import Generator


class OllamaGenerator(Generator):
    def __init__(self):
        super().__init__()
        self.name = "OllamaGenerator"
        self.description = "Generator using a local running Ollama Model specified in the ` OLLAMA_MODEL` variable"
        self.requires_env = ["OLLAMA_URL", "OLLAMA_MODEL"]
        self.streamable = True
        self.context_window = 10000
        self.endpoint = os.environ.get("RUNPOD_ENDPOINT", "")
        runpod.api_key = os.environ.get("RUNPOD_API_KEY","")
        logging.basicConfig(level=logging.DEBUG)


    async def generate_stream(
        self,
        queries: list[str],
        context: list[str],
        conversation: dict = None,
    ):
        """Generate a stream of response dicts based on a list of queries and list of contexts, and includes conversational context
        @parameter: queries : list[str] - List of queries
        @parameter: context : list[str] - List of contexts
        @parameter: conversation : dict - Conversational context
        @returns Iterator[dict] - Token response generated by the Generator in this format {system:TOKEN, finish_reason:stop or empty}.
        """

        url = os.environ.get("OLLAMA_URL", "")
        model = os.environ.get("OLLAMA_MODEL", "")
        if url == "":
            yield {
                "message": "Missing Ollama URL",
                "finish_reason": "stop",
            }

        #disabled when calling through runpod ollama, endpoint determined through "method_name" field in json request
        # url += "/api/chat"

        if conversation is None:
            conversation = {}
        messages = self.prepare_messages(queries, context, conversation)

        try:
            # data = {"model": model, "messages": messages}
            data = {
                    "input": {
                        "method_name": "chat",
                        "input": {
                        "messages": messages
                        }
                    }
                    }
            
            print(f'payload: {data}')
            endpoint = runpod.Endpoint(self.endpoint)
            run_request = endpoint.run(data)
            # Initial check without blocking, useful for quick tasks
            status = run_request.status()
            print(f"Initial job status: {status}")

            if status != "COMPLETED":
                # Polling with timeout for long-running tasks
                output = run_request.output(timeout=60)
            else:
                output = run_request.output()
            print(f"Job output: {output}")
            for line in output.get("message").get("content"):
                if line.strip():  # Ensure line is not just whitespace
                    # json_data = json.loads(
                    #     line.decode("utf-8")
                    # )  # Decode bytes to string then to JSON
                    # output = json_data.get("output")
                    #updated to format from runpod ollama
                    message = output.get("message", {}).get("content", "")
                    finish_reason = (
                        "stop" if output.get("done", False) else ""
                    )

                    yield {
                        "message": message,
                        "finish_reason": finish_reason,
                    }
                else:
                    yield {
                        "message": "",
                        "finish_reason": "stop",
                    }

#runpod SDK - simple async execution
            
#runpod SDK - using asyncio
            # async with aiohttp.ClientSession() as session:
            #     endpoint = AsyncioEndpoint(self.endpoint, session)
            #     job: AsyncioJob = await endpoint.run(data)

            #     # Polling job status
            #     while True:
            #         status = await job.status()
            #         print(f"Current job status: {status}")
            #         if status == "COMPLETED":
            #             output = await job.output()
            #             print("Job output:", output)
            #             break  # Exit the loop once the job is completed.
            #         elif status in ["FAILED"]:
            #             print("Job failed or encountered an error.")

            #             break
            #         else:
            #             print("Job in queue or processing. Waiting 3 seconds...")
            #             await asyncio.sleep(3)  # Wait for 3 seconds before polling again
                
#OLD -
                # async with session.post(url, json=data) as response:
                #     async for line in response.content:
                #         if line.strip():  # Ensure line is not just whitespace
                #             json_data = json.loads(
                #                 line.decode("utf-8")
                #             )  # Decode bytes to string then to JSON
                #             output = json_data.get("output")
                #             #updated to format from runpod ollama
                #             message = output.get("message", {}).get("content", "")
                #             finish_reason = (
                #                 "stop" if output.get("done", False) else ""
                #             )

                #             yield {
                #                 "message": message,
                #                 "finish_reason": finish_reason,
                #             }
                #         else:
                #             yield {
                #                 "message": "",
                #                 "finish_reason": "stop",
                #             }

        except Exception:
            raise

    def prepare_messages(
        self, queries: list[str], context: list[str], conversation: dict[str, str]
    ) -> dict[str, str]:
        """
        Prepares a list of messages formatted for a Retrieval Augmented Generation chatbot system, including system instructions, previous conversation, and a new user query with context.

        @parameter queries: A list of strings representing the user queries to be answered.
        @parameter context: A list of strings representing the context information provided for the queries.
        @parameter conversation: A list of previous conversation messages that include the role and content.

        @returns A list of message dictionaries formatted for the chatbot. This includes an initial system message, the previous conversation messages, and the new user query encapsulated with the provided context.

        Each message in the list is a dictionary with 'role' and 'content' keys, where 'role' is either 'system' or 'user', and 'content' contains the relevant text. This will depend on the LLM used.
        """
        messages = [
            {
                "role": "system",
                "content": self.system_message,
            }
        ]

        for message in conversation:
            messages.append({"role": message.type, "content": message.content})

        query = " ".join(queries)
        user_context = " ".join(context)

        messages.append(
            {
                "role": "user",
                "content": f"With this provided context: '{user_context}' Please answer this query: '{query}'",
            }
        )

        return messages
